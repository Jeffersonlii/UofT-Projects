---
title: "A3"
author: "Jefferson Li, Arib Shaikh"
date: "09/03/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{amsthm}
- \usepackage{accents}
---
## Q1
### a)
```{r}
Data = read.table("PatientSatisfaction.txt",  col.names=c("Y", "X1", "X2", "X3"))

layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
hist(Data$X1,
  main="Patient Age",
  xlab="Age in Years",
  xlim=c(20,55),
)
hist(Data$X2,
  main="Severity of Illness",
  xlab="Index",
  xlim=c(40,65),
)
hist(Data$X3,
  main="Anxiety Level",
  xlab="Index",
  xlim=c(1.5,3),
  breaks=c(1,1.25,1.5,1.75,2,2.25,2.5,2.75,3)

)
```
It is noteworthy that it seems all 3 plots are normally distributed.

### b)

```{r}
# scatter plot matrix

pairs(~Y +X1 + X2+ X3, data = Data, lower.panel = panel.smooth)
cor(cbind(Data$X1, Data$X2, Data$X3))
```
Our scatter plot matrix shows that all 3 predictor value are positively correlated with each other, however, all 3 are negatively correlated with patient satisfaction.

Since none of the correlations between the predictor variables exceed 0.7, the correlations are not extreme enough to raise any concerns of multicollinearity.

### c)
```{r}
fit = lm(Y ~ X1 + X2 + X3, data = Data)
summary(fit)


```

The estimated regression function is 
$$Y = `r fit$coefficients[1]` + `r fit$coefficients[2]` \cdot X_1+ `r fit$coefficients[3]`  \cdot  X_2+ `r fit$coefficients[4]`  \cdot X_3$$
$\hat{\beta}_2$ is interpreted as, controlling for $X_1$ and $X_3$, a 1-unit increase in $X_2$ corresponds to a predicted increase of `r fit$coefficients[2]` in patient satisfaction.

### d)
$$H_0 : \hat{\beta}_1 = \hat{\beta}_2 = \hat{\beta}_3 = 0, \quad H_a : \hat{\beta}_1 \neq 0 \lor \hat{\beta}_2 \neq 0 \lor \hat{\beta}_3 \neq 0$$
Decision rule : reject if $$\text{P Value} = pf(F,3, n-p^\prime) < \alpha$$ then we reject $H_0$ 

```{r}
alpha = 0.10

anova = anova(fit)
anova
SSR =  sum(anova[2][1:3,1])
MSE = anova[3][4,1]

Fstat = SSR/MSE/3
Fstat
```
$$F statistic = `r Fstat`$$

```{r}
Pval = pf(Fstat,4-1, anova[1][4,1],lower.tail=F)
Pval
```

$$\text{Since the P value of }`r Pval` \text{ is less than the level of significance of }\alpha = `r alpha` \text{we can reject }H_0$$
My test implies that we are $90\%$ confident that $\beta_1, \beta_2, \beta_3$ are not all 0.


### e)
```{r}
SSE = anova[2][4,1]
SST = (SSR + SSE)
SSE
R = SSR/SST
R
Radj = 1 - ((anova[1][4,1]+4-1)/anova[1][4,1])*(SSE/SST)
Radj

```
The $R^2$ value of `r R` means that $100 \cdot (`r R`)\%$ of the data fit the regression model.
Furthermore, since the adjusted $R^2$ value is close to $R^2$, we can say most prediction variables are having an effect on predictions. 

### f)
```{r}
#fit
pred = predict(fit, data.frame(X1 = 35, X2 = 45, X3 = 2.2 ) , interval="prediction")
pred
```


I am $95\%$ confident that a future patient with age 35, severity of illness 45, and anxiety level 2.2 will have a patient satisfaction within the range $$[`r pred[1,2]`, `r pred[1,3]`]$$











