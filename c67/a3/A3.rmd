---
title: "A3"
author: "Jefferson Li, Arib Shaikh"
date: "09/03/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{amsthm}
- \usepackage{accents}
---
## Q1
### a)
```{r}
Data = read.table("PatientSatisfaction.txt",  col.names=c("Y", "X1", "X2", "X3"))

layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
hist(Data$X1,
  main="Patient Age",
  xlab="Age in Years",
  xlim=c(20,55),
)
hist(Data$X2,
  main="Severity of Illness",
  xlab="Index",
  xlim=c(40,65),
)
hist(Data$X3,
  main="Anxiety Level",
  xlab="Index",
  xlim=c(1.5,3),
  breaks=c(1,1.25,1.5,1.75,2,2.25,2.5,2.75,3)

)
```
It is noteworthy that it seems all 3 plots are normally distributed.

### b)

```{r}
# scatter plot matrix

pairs(~Y +X1 + X2+ X3, data = Data, lower.panel = panel.smooth)
cor(cbind(Data$X1, Data$X2, Data$X3))
```
Our scatter plot matrix shows that all 3 predictor value are positively correlated with each other, however, all 3 are negatively correlated with patient satisfaction.

Since none of the correlations between the predictor variables exceed 0.7, the correlations are not extreme enough to raise any concerns of multicollinearity.

### c)
```{r}
fit = lm(Y ~ X1 + X2 + X3, data = Data)
summary(fit)


```

The estimated regression function is 
$$Y = `r fit$coefficients[1]` + `r fit$coefficients[2]` \cdot X_1+ `r fit$coefficients[3]`  \cdot  X_2+ `r fit$coefficients[4]`  \cdot X_3$$
$\hat{\beta}_2$ is interpreted as, controlling for $X_1$ and $X_3$, a 1-unit increase in $X_2$ corresponds to a predicted increase of `r fit$coefficients[2]` in patient satisfaction.

### d)
$$H_0 : \hat{\beta}_1 = \hat{\beta}_2 = \hat{\beta}_3 = 0, \quad H_a : \hat{\beta}_1 \neq 0 \lor \hat{\beta}_2 \neq 0 \lor \hat{\beta}_3 \neq 0$$
Decision rule : reject if $$\text{P Value} = pf(F,3, n-p^\prime) < \alpha$$ then we reject $H_0$ 

```{r}
alpha = 0.10

anova = anova(fit)
anova
SSR =  sum(anova[2][1:3,1])
MSE = anova[3][4,1]

Fstat = SSR/MSE/3
Fstat
```
$$F statistic = `r Fstat`$$

```{r}
Pval = pf(Fstat,4-1, anova[1][4,1],lower.tail=F)
Pval
```

$$\text{Since the P value of }`r Pval` \text{ is less than the level of significance of }\alpha = `r alpha` \text{we can reject }H_0$$
My test implies that we are $90\%$ confident that $\beta_1, \beta_2, \beta_3$ are not all 0.


### e)
```{r}
SSE = anova[2][4,1]
SST = (SSR + SSE)
SSE
R = SSR/SST
R
Radj = 1 - ((anova[1][4,1]+4-1)/anova[1][4,1])*(SSE/SST)
Radj

```
The $R^2$ value of `r R` means that $100 \cdot (`r R`)\%$ of the data fit the regression model.
Furthermore, since the adjusted $R^2$ value is close to $R^2$, we can say most prediction variables are having an effect on predictions. 

### f)
```{r}
#fit
pred = predict(fit, data.frame(X1 = 35, X2 = 45, X3 = 2.2 ) , interval="prediction")
pred
```


I am $95\%$ confident that a future patient with age 35, severity of illness 45, and anxiety level 2.2 will have a patient satisfaction within the range $$[`r pred[1,2]`, `r pred[1,3]`]$$


## Q4
### a)

```{r}
Data = read.table("egyptcttn.txt",  col.names=c("Variety", "Luminance", "lnGrade"))
D1 = as.numeric(Data$Variety=="Giza67")
D2 = as.numeric(Data$Variety=="Giza68")
D3 = as.numeric(Data$Variety=="Giza69")
D4 = as.numeric(Data$Variety=="Giza70")

# base = Menoufi

fullFit = lm(Luminance ~ lnGrade*D1 + lnGrade*D2 + lnGrade*D3 + lnGrade*D4, data = Data)
summary(fullFit)

```
Full Model :
\begin{align*}
    Y_i = &\beta_0 + \beta_1 \cdot lnGrade + \beta_2D1 + \beta_3D2 + \beta_4D3 + \beta_5D4 + \\
          & \beta_6 \cdot lnGrade \cdot D1+ \beta_7 \cdot lnGrade \cdot D2+ \beta_8 \cdot lnGrade \cdot D3+ \beta_9 \cdot lnGrade \cdot D4
\end{align*}

### b)


$$H_0 : \beta_6 = \beta_7 =\beta_8 = \beta_9 = 0$$
$$H_a : \text{One of }\beta_6, \beta_7, \beta_8, \beta_9 \text{ is not }0$$

```{r}
reducedFit = lm(Luminance ~ lnGrade + D1 + D2 + D3 + D4, data = Data)
anova = anova(reducedFit, fullFit)
anova
Pval = anova$`Pr(>F)`[2]
Pval
```
since $$\text{P value } = `r Pval` < \alpha =0.05$$
we reject $H_0$.


### c)

$$H_0 :  \beta_2 =\beta_3 =\beta_4 = \beta_5 = \beta_6 = \beta_7 =\beta_8 = \beta_9 = 0$$

$$H_a : \text{One of }\beta_2,\beta_3,\beta_4,\beta_5,\beta_6, \beta_7, \beta_8, \beta_9 \text{ is not }0$$

```{r}
reducedFit = lm(Luminance ~ lnGrade, data = Data)
anova = anova(reducedFit, fullFit)
anova
Pval = anova$`Pr(>F)`[2]
Pval
```
since $$\text{P value } = `r Pval` < \alpha = 0.05$$
we reject $H_0$.


### d)
The model I would choose for this data is the full model, with all dummy variables and interations.
This is because we rejected that the interactions have no effect on slope, so we know the interactions have some significant effects on the model. So we want to use the full model to capture that. 

### e)
```{r}
if(!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

ggplot(data=Data, aes(x=lnGrade, y=Luminance, color= factor(Variety), shape=factor(Variety))) + geom_point() + geom_smooth(method='lm', fill=NA)

```

Normality : 
```{r}
p = shapiro.test(fullFit$residuals)$p.value
```
since $$\text{P value} = `r p` < \alpha = 0.05$$
we reject that this fit satisfies the normality assumption.

Equal Variance :
```{r}
plot(fullFit, which=1)
```
Visually, aside from 1 outlier at(87.7, -0.66), the residuals are uniformly distributed, and  Equal Variance holds.

Linearity : 
```{r}
summary(fullFit)

```
As we can see from the P values, we reject that any of the 5 slopes are 0.
Also, from the graph we can confidently see that there is a linear relationship between luminance and lnGrade for all 5 categories.  
from these 2 observations, we can say our model satisfies linearity.

Independent/uncorrelated error terms : 
```{r}
plot(Data$lnGrade, resid(fullFit), 
     ylab="Residuals", xlab="LnGrade",)
```

From this plot, I visually access that there is no major deviate patterns, therefor 
Independent/uncorrelated error terms is satisfied.



