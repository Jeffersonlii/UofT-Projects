---
title: "A3"
author: "Jefferson Li, Arib Shaikh"
date: "09/03/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{amsthm}
- \usepackage{accents}
---
## Q1
### a)
```{r}
Data = read.table("PatientSatisfaction.txt",  col.names=c("Satisfaction", "Age", "Severity", "Anxiety"))

layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
hist(Data$Age,
  main="Patient Age",
  xlab="Age in Years",
  xlim=c(20,55),
)
hist(Data$Severity,
  main="Severity of Illness",
  xlab="Index",
  xlim=c(40,65),
)
hist(Data$Anxiety,
  main="Anxiety Level",
  xlab="Index",
  xlim=c(1.5,3),
  breaks=c(1,1.25,1.5,1.75,2,2.25,2.5,2.75,3)

)
```
It is noteworthy that it seems all 3 plots are normally distributed.

### b)

```{r}
# scatter plot matrix

pairs(~Satisfaction +Age + Severity+ Anxiety, data = Data, lower.panel = panel.smooth)
cor(cbind(Data$Age, Data$Severity, Data$Anxiety))
```
Our scatter plot matrix shows that all 3 predictor value are positively correlated with each other, however, all 3 are negatively correlated with patient satisfaction.

Since none of the correlations between the predictor variables exceed 0.7, the correlations are not extreme enough to raise any concerns of multicollinearity.

### c)
```{r}
fit = lm(Satisfaction ~ Age + Severity + Anxiety, data = Data)
summary(fit)


```

The estimated regression function is 
$$Y = `r fit$coefficients[1]` + `r fit$coefficients[2]` \cdot X_1+ `r fit$coefficients[3]`  \cdot  X_2+ `r fit$coefficients[4]`  \cdot X_3$$
$\hat{\beta}_2$ is interpreted as, controlling for $X_1$ and $X_3$, a 1-unit increase in $X_2$ corresponds to a predicted increase of `r fit$coefficients[2]` in patient satisfaction.

### d)
$$H_0 : \hat{\beta}_1 = \hat{\beta}_2 = \hat{\beta}_3 = 0, \quad H_a : \hat{\beta}_1 \neq 0 \lor \hat{\beta}_2 \neq 0 \lor \hat{\beta}_3 \neq 0$$
Decision rule : reject if $$\text{P Value} = pf(F,3, n-p^\prime) < \alpha$$ then we reject $H_0$ 

```{r}
alpha = 0.10

anova = anova(fit)
anova
SSR =  sum(anova[2][1:3,1])
MSE = anova[3][4,1]

Fstat = SSR/MSE/3
Fstat
```
$$F statistic = `r Fstat`$$

```{r}
Pval = pf(Fstat,4-1, anova[1][4,1],lower.tail=F)
Pval
```

$$\text{Since the P value of }`r Pval` \text{ is less than the level of significance of }\alpha = `r alpha` \text{we can reject }H_0$$
My test implies that we are $90\%$ confident that $\beta_1, \beta_2, \beta_3$ are not all 0.


### e)
```{r}
SSE = anova[2][4,1]
SST = (SSR + SSE)
SSE
R = SSR/SST
R
Radj = 1 - ((anova[1][4,1]+4-1)/anova[1][4,1])*(SSE/SST)
Radj

```
The $R^2$ value of `r R` means that $100 \cdot (`r R`)\%$ of the data fit the regression model.
Furthermore, since the adjusted $R^2$ value is close to $R^2$, we can say most prediction variables are having an effect on predictions. 

### f)
```{r}
#fit
pred = predict(fit, data.frame(Age = 35, Severity = 45, Anxiety = 2.2 ) , interval="prediction", level = 0.90)
pred
```


I am $90\%$ confident that a future patient with age 35, severity of illness 45, and anxiety level 2.2 will have a patient satisfaction within the range $$[`r pred[1,2]`, `r pred[1,3]`]$$


## Q4
### a)

```{r}
Data = read.table("egyptcttn.txt",  col.names=c("Variety", "Luminance", "lnGrade"))
D1 = as.numeric(Data$Variety=="Giza68")
D2 = as.numeric(Data$Variety=="Giza69")
D3 = as.numeric(Data$Variety=="Giza70")
D4 = as.numeric(Data$Variety=="Menoufi")

# base = Menoufi

fullFit = lm(Luminance ~ lnGrade*D1 + lnGrade*D2 + lnGrade*D3 + lnGrade*D4, data = Data)
summary(fullFit)
ffit = lm(Luminance ~ lnGrade*factor(Data$Variety), data = Data)
summary(ffit)

```
Full Model :
\begin{align*}
    Y_i = &\beta_0 + \beta_1 \cdot lnGrade + \beta_2D1 + \beta_3D2 + \beta_4D3 + \beta_5D4 + \\
          & \beta_6 \cdot lnGrade \cdot D1+ \beta_7 \cdot lnGrade \cdot D2+ \beta_8 \cdot lnGrade \cdot D3+ \beta_9 \cdot lnGrade \cdot D4
\end{align*}

### b)


$$H_0 : \beta_6 = \beta_7 =\beta_8 = \beta_9 = 0$$
$$H_a : \text{One of }\beta_6, \beta_7, \beta_8, \beta_9 \text{ is not }0$$

```{r}
reducedFitB = lm(Luminance ~ lnGrade + D1 + D2 + D3 + D4, data = Data)
anova = anova(reducedFitB, fullFit)
anova
Pval = anova$`Pr(>F)`[2]
Pval
```
since $$\text{P value } = `r Pval` < \alpha =0.05$$
we reject $H_0$.


### c)

$$H_0 :  \beta_2 =\beta_3 =\beta_4 = \beta_5 = \beta_6 = \beta_7 =\beta_8 = \beta_9 = 0$$

$$H_a : \text{One of }\beta_2,\beta_3,\beta_4,\beta_5,\beta_6, \beta_7, \beta_8, \beta_9 \text{ is not }0$$

```{r}
reducedFitC = lm(Luminance ~ lnGrade, data = Data)
anova = anova(reducedFitC, reducedFitB)
anova
Pval = anova$`Pr(>F)`[2]
Pval
```
since $$\text{P value } = `r Pval` < \alpha = 0.05$$
we reject $H_0$.


### d)
The model I would choose for this data is the full model, with all dummy variables and interations.
This is because we rejected that the interactions have no effect on slope, so we know the interactions have some significant effects on the model. So we want to use the full model to capture that. 

### e)
```{r}
if(!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

ggplot(data=Data, aes(x=lnGrade, y=Luminance, color= factor(Variety), shape=factor(Variety))) + geom_point() + geom_smooth(method='lm', fill=NA)

```

Normality : 
```{r}
p = shapiro.test(fullFit$residuals)$p.value
```
since $$\text{P value} = `r p` < \alpha = 0.05$$
we reject that this fit satisfies the normality assumption.

Equal Variance :
```{r}
plot(fullFit, which=1)
```
Visually, aside from 1 outlier at(87.7, -0.66), the residuals are uniformly distributed, and  Equal Variance holds.

Linearity : 
```{r}
summary(fullFit)

```
As we can see from the P values, we reject that any of the 5 slopes are 0.
Also, from the graph we can confidently see that there is a linear relationship between luminance and lnGrade for all 5 categories.  
from these 2 observations, we can say our model satisfies linearity.

Independent/uncorrelated error terms : 
```{r}
plot(Data$lnGrade, resid(fullFit), 
     ylab="Residuals", xlab="LnGrade",)
```

From this plot, I visually access that there is no major deviate patterns, therefor 
Independent/uncorrelated error terms is satisfied.

# Question 5.

```{r}
WoolStrengthData <- read.table("StrengthWool.txt", header = TRUE)
```
## Part a.
Fit a linear model with just main effects for the three class variables and
show that this is not a good fit to the data

```{r}
Cycles = WoolStrengthData$Cycles
lenCut <- cut(WoolStrengthData$Len, breaks=c(0,250, 300, 350), labels = c("250", "300", "350"))
ampCut <- cut(WoolStrengthData$Amp, breaks=c(0,8,9,10), labels = c("8", "9", "10"))
loadCut <- cut(WoolStrengthData$Load, breaks=c(0,40,45,50), labels=c("40", "45", "50"))
fit <- lm(Cycles ~ lenCut + ampCut + loadCut, data = WoolStrengthData)
summary(fit)

residual <- fit$residuals
fitted <- fit$fitted.values
plot(x=fitted, y=residual)
qqnorm(residual)
qqline(residual)
```
If we look at the plot of the Residual Plot as well as the QQ Plot of the model, we can see that this model is not a good fit to the data, where the QQ Plot shows many points not following the linear line, as well as the curved shape in the residual plot not showing signs of a good fit.

#5.b)
```{r}

new_fit <- lm(Cycles ~ ampCut + lenCut + loadCut + ampCut*lenCut + ampCut*loadCut + lenCut*loadCut, data = WoolStrengthData)
summary(new_fit)
new_residual <- new_fit$residuals
new_fitted <- new_fit$fitted.values
plot(x=new_fitted, y=new_residual)
qqnorm(new_residual)
qqline(new_residual)

```
If we look at the new residual plot of the new model, we can see that this model is a better fit to the data, as we can see that the variance is linear around 0 but does not have constant variance. Hence, the model which considers all the interactions between pairs of the class variables is a better model for fitting the data.

## Part c.
```{r}
transformed_fit <- lm(log(Cycles)~(lenCut + ampCut + loadCut))

summary(transformed_fit)
transformed_residual <- transformed_fit$residuals
transformed_fitted <- transformed_fit$fitted.values
plot(x=transformed_fitted, y=transformed_residual)
qqnorm(transformed_residual)
qqline(transformed_residual)
```

After performing the log transformation on the model, we can see that the variance is constant and linear around 0. The high R/Radj values also show that almost all of the variance can be explained by the model. This is a better fit for the model.

## Part d.
```{r}
transformed_fit2 <- lm(log(Cycles)~(ampCut + lenCut + loadCut + ampCut*lenCut + ampCut*loadCut + lenCut*loadCut))

summary(transformed_fit2)
transformed_residual <- transformed_fit2$residuals
transformed_fitted <- transformed_fit2$fitted.values
plot(x=transformed_fitted, y=transformed_residual)
qqnorm(transformed_residual)
qqline(transformed_residual)
anova(transformed_fit, transformed_fit2)
```

We can see that the result of this is very similar to that of part c. However, it is not a significant change in terms of deciding a better fit, and we can use F-test. Since the p-value of F-test is 0.1325 > 0.05, With 95% confidence we cannot reject H0 as there is evidence that the interaction terms have no effect on the model with transformations