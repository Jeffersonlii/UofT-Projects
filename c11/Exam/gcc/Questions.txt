"""
CSCC11 - Introduction to Machine Learning, Fall 2020, Exam
B. Chan, D. Fleet
"""

Gaussian Class Conditionals

Answer The Following Questions:
1. Generic 1: Does GCC do well? Why or why not?
 GCC performs very well on generic 1,
   the data is clearly separated,
   and the  data looks like it was generated by a Gaussian.

2. Generic 2: Does GCC do well? Why or why not?
  GCC does perform well,
  the data looks like its been generated by a Gaussian, and the 2 classes are nicely separated.
  Althought, one of the Gaussians is within another, this doesnt matter since there is still a distinct
  Decision Boundary

3. Generic 3: Does GCC do well? Why or why not?
   GCC does not do well,
   the 2 classes are very overlapping, making it hard for 2 Gaussian's to capture the model,
   the data also doesnt seem to be generated from a gaussian distribution, as they seem to have a
   a parabolic shape.

4. If input feature vectors have length D, how many parameters do we need to learn for each Gaussian?
    1 (prior) + D (means) + D * D (covariance) = 1 + D + D * D

5. Suppose you decide to use Naive Bayes with GCC. In words, what would you have to change in your code?
   How many parameters would you need to learn for each Gaussian in this case?

   The main change would be how we calculate the posterior, instead of calculating 1 d dimensional multivariate
   Gaussian distribution, we calculate d 1 dimensional univariate
   Gaussian distributions. Our naive bayes posterior is much simpler, needing to only learn
   1 + D + D parameters

6. Suppose certain elements of the input feature vectors are highly correlated.
   Why is this problematic for Naive Bayes?
    Naive Bayes can not model correlation, as its training speed comes from ignoring the covariance.
    I expect its prediction performance to suffer.

7. How could you transform the data to mitigate this problem with Naive Bayes?
    We can look at the correlation of each pair of features, and remove 1/2 features
    for pairs that are very correlated.

    for example if the dataset is for driving accidents,
    we have deaths, drinking, and chewing gum.
    We can see deaths and drinking are very correlated, we can remove one of the features from the data.



