"""
CSCC11 - Introduction to Machine Learning, Fall 2020, Assignment 4 - Dimensionality Reduction
B. Chan, S. Wei, D. Fleet
"""

Answer The Following Questions:

Understanding Eigenvalues:
1. How do the eigenvalues relate to the diagonal variances used in generating the data?
There is an inversely proportional relationship

2. How does the additive noise affect the curves?
lower noise horizontally compresses the curve and shifts it to the left.
in other words, lower noise relatively increases the difference between each consecutive eigen value.

3. Based on the plots, can you hypothesize the ways to choose the number of subspace dimensions?
choose the lowest subspace dimension able to capture 95% of the variance.
in other words, choose the lowest subspace dimension equal or above 0.95 on the fraction (y) axis.


PCA on document data:
1. How big is the covariance matrix used in the PCA algorithm?
9635 by 9635

2. How long does PCA algorithm take?
101.62s

3. Do the points from different classes look reasonably separable in this space?
no, they look very concentrated at one volume of space
#todo

EM-PCA v.s. PCA:
1. After running visualize_documents.py, compare the parameters you've estimated using PCA and EM-PCA. Are they identical and if not, how do they differ?
They are not identical, in fact, they seem to have flipped signs with respect to each other.
This is fine, In PCA, we only really care about the Principle component, which is simply a line that is direction agnostic.
A vector with every component negated will still lie on the same line, and therefore the principle component is effectively the same.

2. Which algorithm takes more space?
PCA takes more space, as EM-PCA doesnt calculate a huge matrix that is stored in memory.

3. How long does EM-PCA algorithm take to run compared to PCA?
EM-PCA takes 7 seconds, much shorter compared to the 101 seconds of PCA


